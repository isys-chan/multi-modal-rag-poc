{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c1b998",
   "metadata": {},
   "source": [
    "# chroma basic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba271e",
   "metadata": {},
   "source": [
    "## chroma setup\n",
    "(already set, so commented out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0212f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import chromadb\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\") # PersistentClientにすると、DBがfolder保存される\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb1b23a",
   "metadata": {},
   "source": [
    "## OpenAI enbedding setup\n",
    "open ai APIs now do not support multi modal embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b307164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1536)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from chromadb.utils import embedding_functions\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "# openai_mm = embedding_functions.OpenAIEmbeddingFunction(\n",
    "#     api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "#     model_name=\"text-embedding-3-small  mm embedding model\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927435a6",
   "metadata": {},
   "source": [
    "## OpenCLIP setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f8ed705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishid\\miniconda3\\envs\\airizap\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\ishid\\miniconda3\\envs\\airizap\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ishid\\.cache\\huggingface\\hub\\models--laion--CLIP-ViT-B-32-laion2B-s34B-b79K. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction\n",
    "embedding_function = OpenCLIPEmbeddingFunction()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64554f4",
   "metadata": {},
   "source": [
    "## setting collection(=table in RDB)\n",
    "Chromaフォルダ内でUUIDで識別される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f736c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import chromadb\n",
    "from chromadb.utils.data_loaders import ImageLoader\n",
    "from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction\n",
    "\n",
    "data_loader = ImageLoader() # OpenCLIPでは直接画像をChroma DBに保存しない。そのため、URI指定で生画像を持ってきてくれるImage Loaderを使用する。\n",
    "embedding_function = OpenCLIPEmbeddingFunction()\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name='multimodal_collection',\n",
    "    embedding_function=embedding_function,\n",
    "    data_loader=data_loader\n",
    ")\n",
    "\n",
    "# if you want to make multi modal vector database, you have two options:\n",
    "# 1. use image -> captions model, then use caption to text embedding model to store chrome db.\n",
    "# 2. use CLIP(Contrastive Language–Image Pretraining) embedding model to directly embed image and text into same vector space. (to be studied)\n",
    "# CLIP leasrns a lot of image-text pairs. it is used for image search and classification, generation etc.(like DALL-E, Stable Diffusion etc.)\n",
    "\n",
    "# 1. を使う場合、ベクトルDBに追加するデータは細切れに、元データは大枠に保存するらしい。ベクター検索は細切れのほうが精度がよく、コンテキストとして使うには大枠のほうがいいため。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04fe094",
   "metadata": {},
   "source": [
    "for multi modal db, i refered chroma official document: \n",
    "[Embeddings - Multimodal](https://docs.trychroma.com/docs/embeddings/multimodal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6a12f5",
   "metadata": {},
   "source": [
    "## pdf processer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad7a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to deal with the pdf, you have to;\n",
    "# - 1. loard pdf,\n",
    "# - 2. split the pdf into chunks, \n",
    "# - 3. embed the chunks, \n",
    "# - 4. store the embedded chunks into chroma db.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8733bac",
   "metadata": {},
   "source": [
    "[REF](https://dev.nagomi-tec.com/archives/2859)\n",
    "\n",
    "[PDF textスプリットはこれが参考になるかも](https://zenn.dev/ml_bear/books/d1f060a3f166a5/viewer/e1085f)\n",
    "\n",
    "https://qiita.com/camcam/items/ae9ac4860968389804bd\n",
    "PDF loader選びが大切そう"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76bf6eb",
   "metadata": {},
   "source": [
    "## adding document to collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1844a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding image file paths and text documents to collection\n",
    "collection.add(\n",
    "    ids=[\"id1\", \"id2\"],\n",
    "    uris=[\"path/to/file/1\", \"path/to/file/2\"]\n",
    ")\n",
    "\n",
    "collection.add(\n",
    "    ids=[\"id3\", \"id4\"],\n",
    "    documents=[\"This is a document\", \"This is another document\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f3e9a",
   "metadata": {},
   "source": [
    "## seraching DB and result extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17f1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['doc1']], 'embeddings': None, 'documents': [['This is a sample document.']], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[{'source': 'sample_source'}]], 'distances': [[0.8754018545150757]]}\n",
      "---------------\n",
      "This is a sample document.\n"
     ]
    }
   ],
   "source": [
    "# seraching collection\n",
    "query = \"sample\"\n",
    "results = collection.query(\n",
    "    query_texts=[query], # 検索クエリ(内部的にquery_textをembeddingしてベクトル検索を行う。）\n",
    "    # query_embeddingsで直接ベクトルを検索をすることもできる。ただし、次元数が合わないとエラーになるなど注意が必要)\n",
    "    # AZUREopenaiみたいにハイブリッド検索はできない（難しい）けど、無料でここまでできるならすごい\n",
    "    n_results=3 # 返す件数\n",
    "    include=[\"documents\",\"data\"] # 返す内容指定. dataを入れるとData loaderが自動的に呼び出され、（画像などの）データも返す。\n",
    "    # where={} でメタデータ絞り込みも可能\n",
    "    )\n",
    "print(results)\n",
    "print(\"---------------\")\n",
    "print(results[\"documents\"][0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airizap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
